{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "\n",
    "\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "\n",
    "def caption2str(caption):\n",
    "    # caption is a list of word \"ids\"\n",
    "    return ' '.join(id2word_dict[str(i)] \\\n",
    "                    for i in caption \\\n",
    "                    if str(i) in id2word_dict and id2word_dict[str(i)][0] != '<')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset by Dataset API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "def preprocess_data(image_path, caption):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    \n",
    "    img = tf.cast(img, tf.float32)\n",
    "    \n",
    "    # data augmentation\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.04)\n",
    "    img = tf.image.resize(img, (IMAGE_SIZE + IMAGE_SIZE // 10, IMAGE_SIZE + IMAGE_SIZE // 10))\n",
    "    img = tf.image.random_crop(img, (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    \n",
    "    img = (img / 255) * 2 - 1\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(pkl_file):\n",
    "    df = pd.read_pickle(pkl_file)\n",
    "    captions = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    caption = []\n",
    "    image_path = []\n",
    "    for i, caps in enumerate(captions):\n",
    "        for cap in caps:\n",
    "            caption.append(cap)\n",
    "            image_path.append(image_paths[i])\n",
    "            \n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int32)\n",
    "    caption = [caption2str(cap) for cap in caption]\n",
    "    caption = tf.concat(\n",
    "        [text_encoder(caption[64 * i:64 * min(len(caption), i + 1)]) \\\n",
    "             for i in range((len(caption) + 63) // 64)], \n",
    "        axis=0)\n",
    "    caption = caption.numpy()\n",
    "    caption = np.asarray(caption)\n",
    "    caption = np.concatenate([caption] * NUM_CAP_PER_IMG, axis=0)\n",
    "    image_path = np.concatenate([image_path] * NUM_CAP_PER_IMG, axis=0)\n",
    "    \n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_path, caption))\n",
    "    dataset = dataset.map(preprocess_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl')\n",
    "num_steps = len(dataset)\n",
    "print(f'Num steps: {num_steps}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text ENcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFSentenceTransformer(keras.layers.Layer):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def call(self, inputs, normalize=True):\n",
    "        model_output = self.model(inputs)\n",
    "        embeddings = self.mean_pooling(model_output, inputs['attention_mask'])\n",
    "        if normalize:\n",
    "            embeddings = self.normalize(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = tf.cast(\n",
    "            tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)),\n",
    "            tf.float32\n",
    "        )\n",
    "        return tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) \\\n",
    "                / tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)\n",
    "\n",
    "    def normalize(self, embeddings):\n",
    "        embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class E2ESentenceTransformer(keras.Model):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(name='text_encoder')\n",
    "#         self.tokenizer = TFBertTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFSentenceTransformer(model_name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "#         tokenized = self.tokenizer(inputs)\n",
    "        tokenized = self.tokenizer(inputs, padding=True, truncation=True, return_tensors='tf')\n",
    "        return self.model(tokenized)\n",
    "    \n",
    "\n",
    "# # model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "# text_encoder = E2ESentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self, input_z_shape, emb_shape):\n",
    "        super().__init__(name='generator')\n",
    "        self.input_z_shape = input_z_shape\n",
    "        self.emb_shape = emb_shape\n",
    "        self.text = keras.Sequential([\n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(384), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "        ])\n",
    "        self.generator = keras.Sequential([\n",
    "            keras.layers.Dense(8192, use_bias=False), \n",
    "            keras.layers.Reshape((4, 4, 512)), \n",
    "    \n",
    "            keras.layers.Conv2DTranspose(\n",
    "                512, \n",
    "                (4, 4), \n",
    "                strides=(1, 1), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2DTranspose(\n",
    "                256, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "            keras.layers.Conv2DTranspose(\n",
    "                128, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2DTranspose(\n",
    "                64, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2DTranspose(\n",
    "                3, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                activation='tanh', \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "        ])\n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.text(text)\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        output = self.generator(text_concat)\n",
    "        return output\n",
    "\n",
    "    def summary(self):\n",
    "        text = keras.layers.Input(shape=(self.emb_shape, ), name='text')\n",
    "        noise_z = keras.layers.Input(shape=(self.input_z_shape, ), name='noise_z')\n",
    "        model = keras.Model(name='generator', inputs=[text, noise_z], outputs=self.call(text, noise_z))\n",
    "        return model.summary()\n",
    "\n",
    "# generator = Generator(Z_DIM, EMB_DIM)\n",
    "# generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, input_x_shape, emb_shape):\n",
    "        super().__init__(name='generator')\n",
    "        self.input_x_shape = input_x_shape\n",
    "        self.emb_shape = emb_shape\n",
    "        self.text = keras.Sequential([\n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(384), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "        ])\n",
    "        self.image = keras.Sequential([\n",
    "            keras.layers.Conv2D(\n",
    "                64, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2D(\n",
    "                128, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2D(\n",
    "                256, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "            keras.layers.Conv2D(\n",
    "                512, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "        ])\n",
    "        self.discriminator = keras.Sequential([\n",
    "            keras.layers.Conv2D(\n",
    "                512, \n",
    "                (1, 1), \n",
    "                strides=(1, 1), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(),\n",
    "            \n",
    "            keras.layers.Conv2D(\n",
    "                1, \n",
    "                (4, 4), \n",
    "                strides=(1, 1), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(1), \n",
    "        ])\n",
    "        \n",
    "    def call(self, text, img):\n",
    "        text = self.text(text)\n",
    "        text = tf.reshape(text, shape=(-1, 4, 4, text.shape[-1] // 16))\n",
    "        img = self.image(img)\n",
    "        text_concat = tf.concat([img, text], axis=3)\n",
    "        output = self.discriminator(text_concat)\n",
    "        return output\n",
    "\n",
    "    def summary(self):\n",
    "        text = keras.layers.Input(shape=(self.emb_shape, ), name='text')\n",
    "        img = keras.layers.Input(shape=self.input_x_shape, name='img')\n",
    "        model = keras.Model(name='discriminator', inputs=[text, img], outputs=self.call(text, img))\n",
    "        return model.summary()\n",
    "\n",
    "# discriminator = Discriminator(IMAGE_SHAPE, EMB_DIM)\n",
    "# discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters\n",
    "wrap in h{}?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM = 128\n",
    "EMB_DIM = 768\n",
    "IMAGE_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "NUM_CAP_PER_IMG = 3\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "LAMBDA = 10\n",
    "EPOCHS = 500\n",
    "EPOCHS_PER_CKPT = 5\n",
    "N_CRITIC = 3\n",
    "\n",
    "SAMPLE_ROW = 3\n",
    "SAMPLE_COL = 4\n",
    "SAMPLE_NUM = SAMPLE_ROW * SAMPLE_COL\n",
    "SAMPLE_DURATION = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "text_encoder = E2ESentenceTransformer(model_name)\n",
    "\n",
    "generator = Generator(Z_DIM, EMB_DIM)\n",
    "generator.summary()\n",
    "\n",
    "discriminator = Discriminator(IMAGE_SHAPE, EMB_DIM)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS & OPTIMIze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "optimizer_d = keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt-bert3'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator=generator,\n",
    "                           discriminator=discriminator,\n",
    "                           optimizer_g=optimizer_g,\n",
    "                           optimizer_d=optimizer_d)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "start_epoch = 1\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    latest_ckpt = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    start_epoch = EPOCHS_PER_CKPT * latest_ckpt + 1\n",
    "    print(f'Restore from latest checkpoint: {ckpt_manager.latest_checkpoint.split(\"/\")[-1]}')\n",
    "    \n",
    "print(f'Start epoch: {start_epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_x_list = []\n",
    "loss_g_list = []\n",
    "loss_d_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_g(real_img, caption):\n",
    "    input_g = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "    text = caption\n",
    "    \n",
    "    with tf.GradientTape() as tape_g:\n",
    "        fake_img = generator(text, input_g, training=True)\n",
    "        fake_pred = discriminator(text, fake_img, training=True)\n",
    "        loss_g = -tf.reduce_mean(fake_pred)\n",
    "        \n",
    "    gradient_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "    optimizer_g.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "    \n",
    "    return loss_g\n",
    "    \n",
    "@tf.function\n",
    "def train_step_d(real_img, caption):\n",
    "    input_g = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "    text = caption\n",
    "    epsilon = tf.random.uniform(shape=[BATCH_SIZE, 1, 1, 1], minval=0, maxval=1)\n",
    "    \n",
    "    with tf.GradientTape() as tape_d:\n",
    "        with tf.GradientTape() as tape_gp:\n",
    "            fake_img = generator(text, input_g, training=True)\n",
    "            fake_img_gp = epsilon * real_img + (1 - epsilon) * fake_img\n",
    "            fake_pred_gp = discriminator(text, fake_img_gp, training=True)\n",
    "        \n",
    "        gradient_gp = tape_gp.gradient(fake_pred_gp, fake_img_gp)\n",
    "        gradient_norm_gp = tf.sqrt(tf.reduce_sum(tf.square(gradient_gp), axis=np.arange(1, len(gradient_gp.shape))))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(gradient_norm_gp - 1))\n",
    "        \n",
    "        fake_pred = discriminator(text, fake_img, training=True)\n",
    "        real_pred = discriminator(text, real_img, training=True)\n",
    "        \n",
    "        loss_d = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred) + LAMBDA * gradient_penalty\n",
    "    \n",
    "    gradient_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "    optimizer_d.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return loss_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption):\n",
    "    input_g = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "    text = caption\n",
    "    fake_img = generator(text, input_g, training=False)\n",
    "    return fake_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuala5toin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(imgs, row, col, path=None):\n",
    "    h, w, c = imgs[0].shape\n",
    "    out = np.zeros((h * row, w * col, c), dtype=np.uint8)\n",
    "    for n, img in enumerate(imgs):\n",
    "        j, i = divmod(n, col)\n",
    "        out[j * h : (j + 1) * h, i * w : (i + 1) * w, :] = img\n",
    "    if path is not None: \n",
    "        imageio.imwrite(path, out)\n",
    "    return out\n",
    "  \n",
    "def generate_gif(imgs_path_list, fname, duration):\n",
    "    imgs = []\n",
    "    for img_path in imgs_path_list:\n",
    "        img = imageio.imread(img_path)\n",
    "        img_id = img_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        img_with_id = imageio.core.util.Array(np.concatenate([np.ones((20, img.shape[1], 3), dtype=np.uint8) * 255, img], axis=0))\n",
    "        img_with_id[:20, :, :] = 0\n",
    "        img_with_id[:20, :, 0] = 255\n",
    "        img_with_id[:20, :, 1] = 255\n",
    "        img_with_id[:20, :, 2] = 255\n",
    "        img_with_id[:20, :, :] = cv2.putText(img_with_id[:20, :, :], f'EPOCH: {img_id}', (10, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        imgs.append(img_with_id)\n",
    "        \n",
    "    n = float(len(imgs)) / duration\n",
    "    clip = mpy.VideoClip(lambda t: imgs[int(n * t)], duration=duration)\n",
    "    clip.write_gif(fname, fps=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cap = [\n",
    "    'flower with white long white petals and very long purple stamen ',\n",
    "    'this medium white flower has rows of thin blue petals and thick stamen ',\n",
    "    'this flower is white and purple in color with petals that are oval shaped ',\n",
    "    'this flower is pink and yellow in color with petals that are oval shaped ',\n",
    "    'the flower has a large bright orange petal with pink anther ',\n",
    "    'the flower shown has a smooth white petal with patches of yellow as well ',\n",
    "    'white petals that become yellow as they go to the center where there is an orange stamen ',\n",
    "    'this flower has bright red petals with green pedicel as its main features ',\n",
    "    'this flower has the overlapping yellow petals arranged closely toward the center ',\n",
    "    'this flower has green sepals surrounding several layers of slightly ruffled pink petals ',\n",
    "    'the pedicel on this flower is purple with a green sepal and rose colored petals ',\n",
    "    'this white flower has connected circular petals with yellow stamen ',\n",
    "    'the flower has yellow petals overlapping each other and are yellow in color ',\n",
    "    'this flower has numerous stamen ringed by multiple layers of thin pink petals ',\n",
    "    'the petals are broad but thin at the edges with purple tints at the edges and white in the middle ',\n",
    "    'the yellow flower has petals that are soft smooth and arranged in two layers below the bunch of stamen ',\n",
    "    'this flower has petals that are pink and yellow with yellow stamen ',\n",
    "    'red stacked petals surround yellow stamen and a black pistil ',\n",
    "    'the petals of the flower are in multiple layers and are pink in yellow in color ',\n",
    "    'this flower has a yellow center and layers of peach colored petals with pointed tips ',\n",
    "    'this bright pink flower has several fluttery petals and a tubular center ',\n",
    "    'this flower is white and yellow in color with petals that are rounded at the endges ',\n",
    "    'the flower has a several pieces of yellow colored petals that looks similar to its leaves ',\n",
    "    'this flower has several light pink petals and yellow anthers ',\n",
    "    'this flower is yellow and white in color with petals that are star shaped near the cener ',\n",
    "    'lavender and white pedal and yellow small flower in the middle of the pedals ',\n",
    "    'this flower has lavender petals with maroon stripes and brown anther filaments ',\n",
    "    'this flower has six plain pale yellow petals that alternate with three dark yellow speckled petals ',\n",
    "    'this flower has petals that are yellow with orange lines ',\n",
    "    'the flower has petals that are orange with yellow stamen ',\n",
    "    'this flower has a brown center surrounded by layers of long yellow petals with rounded tips ',\n",
    "    'this flower is lavender in color with petals that are ruffled and wavy ',\n",
    "    'this flower is blue in color with petals that have veins ',\n",
    "    'the petals on this flower are white with yellow stamen ',\n",
    "    'this flower has petals that are cone shaped and dark purple ',\n",
    "    'this flower is purple and white in color and has petals that are multi colored ',\n",
    "    'a large group of bells that are blue on this flower ',\n",
    "    'this flower is bright purple with many pedals that are roundish whth pale white outer petals ',\n",
    "    'this flower has large yellow petals and long yellow stamen on it ',\n",
    "    'this flower has spiky blue petals and a spiky black stigma on it ',\n",
    "    'this flower is purple and yellow in color with petals that are oval shaped ',\n",
    "    'the flower has petals that are large and pink with yellow anther',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample gen???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sample_z = tf.random.normal([SAMPLE_NUM, Z_DIM])\n",
    "sample_cap = test_cap[:SAMPLE_NUM]\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    \n",
    "    loss_g = 0\n",
    "    loss_d = 0\n",
    "    \n",
    "    # Train step\n",
    "    for step, (img, cap) in tqdm(enumerate(dataset), total=num_steps):\n",
    "        if step % (N_CRITIC + 1) == N_CRITIC - 1:\n",
    "            loss_g += train_step_g(img, cap)\n",
    "            # Store data to list\n",
    "            loss_x_list.append(epoch + step / num_steps)\n",
    "            loss_g_list.append(loss_g / 1)\n",
    "            loss_d_list.append(loss_d / N_CRITIC)\n",
    "            loss_g = 0\n",
    "            loss_d = 0\n",
    "        else:\n",
    "            loss_d += train_step_d(img, cap)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch % EPOCHS_PER_CKPT == 0:\n",
    "        ckpt_manager.save()\n",
    "    \n",
    "    # Generate sample image\n",
    "    sample_text = text_encoder(sample_cap)\n",
    "    sample_imgs = generator(sample_text, sample_z, training=False)\n",
    "    sample_imgs = tf.clip_by_value((sample_imgs + 1) / 2 * 255, 0, 255)\n",
    "    img = generate_img(sample_imgs, SAMPLE_ROW, SAMPLE_COL, f'imgs/{epoch:>04d}.png')\n",
    "    \n",
    "    # Display sample image\n",
    "    if epoch % 5 == 0:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(f'Epoch {epoch}: Loss_g {loss_g_list[-1]:.5f}, Loss_d {loss_d_list[-1]:.5f}')\n",
    "    \n",
    "print (f'Time taken for {EPOCHS} epoch: {time.time() - start} sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess_data(index, caption):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return index, caption\n",
    "\n",
    "def test_dataset_generator(pkl_file):\n",
    "    df = pd.read_pickle(pkl_file)\n",
    "    captions = df['Captions'].values\n",
    "    caption = np.asarray(captions)\n",
    "#     caption = caption.astype(np.int32)\n",
    "    caption = [caption2str(x) for x in caption]\n",
    "    caption = tf.concat(\n",
    "        [text_encoder(caption[64 * i:64 * min(len(caption), i + 1)]) \\\n",
    "             for i in range((len(caption) + 63) // 64)], \n",
    "        axis=0)\n",
    "    caption = caption.numpy()\n",
    "    caption = np.asarray(caption)\n",
    "    index = df['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    assert caption.shape[0] == index.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((index, caption))\n",
    "    dataset = dataset.map(test_preprocess_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(BATCH_SIZE, drop_remainder=True)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = test_dataset_generator(data_path + '/testData.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_pickle(data_path + '/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './inference/demo'\n",
    "\n",
    "def inference(dataset):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for idx, captions in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions)\n",
    "        step += 1\n",
    "        for i in range(BATCH_SIZE):\n",
    "            img = np.clip(fake_image[i].numpy() * 0.5 + 0.5, 0.0, 1.0)\n",
    "            plt.imsave(output_dir + '/inference_{:04d}.jpg'.format(idx[i]), img)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time() - start))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
