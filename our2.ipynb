{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display\n",
    "\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2WordList(caption):\n",
    "    return ' '.join(id2word_dict[str(i)] \\\n",
    "                    for i in caption \\\n",
    "                    if str(i) in id2word_dict and id2word_dict[str(i)][0] != '<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent2WordList(sent2IdList(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFSentenceTransformer(keras.layers.Layer):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def call(self, inputs, normalize=True):\n",
    "        model_output = self.model(inputs)\n",
    "        embeddings = self.mean_pooling(model_output, inputs['attention_mask'])\n",
    "        if normalize:\n",
    "            embeddings = self.normalize(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = tf.cast(\n",
    "            tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)),\n",
    "            tf.float32\n",
    "        )\n",
    "        return tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) \\\n",
    "                / tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)\n",
    "\n",
    "    def normalize(self, embeddings):\n",
    "        embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class E2ESentenceTransformer(keras.Model):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(name='text_encoder')\n",
    "#         self.tokenizer = TFBertTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFSentenceTransformer(model_name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "#         tokenized = self.tokenizer(inputs)\n",
    "        tokenized = self.tokenizer(inputs, padding=True, truncation=True, return_tensors='tf')\n",
    "        return self.model(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self, input_z_shape, emb_shape):\n",
    "        super().__init__(name='generator')\n",
    "        self.input_z_shape = input_z_shape\n",
    "        self.emb_shape = emb_shape\n",
    "        self.text = keras.Sequential([\n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(384), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "        ])\n",
    "        self.generator = keras.Sequential([\n",
    "            keras.layers.Dense(8192, use_bias=False), \n",
    "            keras.layers.Reshape((4, 4, 512)), \n",
    "    \n",
    "            keras.layers.Conv2DTranspose(\n",
    "                512, \n",
    "                (4, 4), \n",
    "                strides=(1, 1), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2DTranspose(\n",
    "                256, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "            keras.layers.Conv2DTranspose(\n",
    "                128, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2DTranspose(\n",
    "                64, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2DTranspose(\n",
    "                3, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                activation='tanh', \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "        ])\n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.text(text)\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        output = self.generator(text_concat)\n",
    "        return output\n",
    "\n",
    "    def summary(self):\n",
    "        text = keras.layers.Input(shape=(self.emb_shape, ), name='text')\n",
    "        noise_z = keras.layers.Input(shape=(self.input_z_shape, ), name='noise_z')\n",
    "        model = keras.Model(name='generator', inputs=[text, noise_z], outputs=self.call(text, noise_z))\n",
    "        return model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, input_x_shape, emb_shape):\n",
    "        super().__init__(name='generator')\n",
    "        self.input_x_shape = input_x_shape\n",
    "        self.emb_shape = emb_shape\n",
    "        self.text = keras.Sequential([\n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(384), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            keras.layers.LeakyReLU(), \n",
    "        ])\n",
    "        self.image = keras.Sequential([\n",
    "            keras.layers.Conv2D(\n",
    "                64, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2D(\n",
    "                128, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "\n",
    "            keras.layers.Conv2D(\n",
    "                256, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "            keras.layers.Conv2D(\n",
    "                512, \n",
    "                (4, 4), \n",
    "                strides=(2, 2), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(), \n",
    "        ])\n",
    "        self.discriminator = keras.Sequential([\n",
    "            keras.layers.Conv2D(\n",
    "                512, \n",
    "                (1, 1), \n",
    "                strides=(1, 1), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.LeakyReLU(),\n",
    "            \n",
    "            keras.layers.Conv2D(\n",
    "                1, \n",
    "                (4, 4), \n",
    "                strides=(1, 1), \n",
    "                padding='same', \n",
    "                use_bias=False, \n",
    "                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "            ), \n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(1), \n",
    "        ])\n",
    "        \n",
    "    def call(self, text, img):\n",
    "        text = self.text(text)\n",
    "        text = tf.reshape(text, shape=(-1, 4, 4, text.shape[-1] // 16))\n",
    "        img = self.image(img)\n",
    "        text_concat = tf.concat([img, text], axis=3)\n",
    "        output = self.discriminator(text_concat)\n",
    "        return output\n",
    "\n",
    "    def summary(self):\n",
    "        text = keras.layers.Input(shape=(self.emb_shape, ), name='text')\n",
    "        img = keras.layers.Input(shape=self.input_x_shape, name='img')\n",
    "        model = keras.Model(name='discriminator', inputs=[text, img], outputs=self.call(text, img))\n",
    "        return model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "EMBED_DIM = 768\n",
    "Z_DIM = 128\n",
    "IMAGE_SIZE = [64, 64, 3]\n",
    "\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = E2ESentenceTransformer(MODEL_NAME)\n",
    "generator = Generator(Z_DIM, EMBED_DIM)\n",
    "discriminator = Discriminator(IMAGE_SIZE, EMBED_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "NUM_CAP_PER_IMG = 3\n",
    "\n",
    "def training_data_generator_aug(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # data augmentation\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.04)\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT + IMAGE_HEIGHT // 10, IMAGE_WIDTH + IMAGE_WIDTH // 10])\n",
    "    img = tf.image.random_crop(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = (img / 255) * 2 - 1\n",
    "    # caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, num_rows=None):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    captions = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "\n",
    "    caption = []\n",
    "    image_path = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose all of them randomly for training\n",
    "    for idx in range(len(captions)):\n",
    "        for cap in captions[idx]:\n",
    "            caption.append(cap)\n",
    "            image_path.append(image_paths[idx])\n",
    "    caption = np.asarray(caption)\n",
    "    # caption = caption.astype(np.int)\n",
    "    caption = [sent2WordList(cap) for cap in caption]\n",
    "    # print(caption[:5])\n",
    "    caption = tf.concat(\n",
    "        [text_encoder(caption[64 * i:64 * min(len(caption), i + 1)]) \\\n",
    "             for i in range((len(caption) + 63) // 64)], \n",
    "        axis=0)\n",
    "    caption = caption.numpy()\n",
    "    caption = np.asarray(caption)\n",
    "    caption = np.concatenate([caption] * NUM_CAP_PER_IMG, axis=0)\n",
    "    image_path = np.concatenate([image_path] * NUM_CAP_PER_IMG, axis=0)\n",
    "\n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "BATCH_SIZE = 8\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator_aug, num_rows=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "SAMPLE_ROW = 3\n",
    "SAMPLE_COL = 4\n",
    "SAMPLE_NUM = SAMPLE_ROW * SAMPLE_COL\n",
    "\n",
    "def generate_img(imgs, row, col, path=None):\n",
    "    h, w, c = imgs[0].shape\n",
    "    out = np.zeros((h * row, w * col, c), dtype=np.uint8)\n",
    "    for n, img in enumerate(imgs):\n",
    "        j, i = divmod(n, col)\n",
    "        out[j * h : (j + 1) * h, i * w : (i + 1) * w, :] = img\n",
    "    if path is not None: \n",
    "        imageio.imwrite(path, out)\n",
    "    return out\n",
    "\n",
    "num_steps = len(dataset)\n",
    "print(f'Num steps: {num_steps}')\n",
    "\n",
    "imgs, caps = next(iter(dataset))\n",
    "\n",
    "# Show caption\n",
    "# caps = caps[:SAMPLE_NUM].numpy()\n",
    "# for idx, cap in enumerate(caps):\n",
    "#     i, j = divmod(idx, SAMPLE_COL)\n",
    "#     print(f'({i+1},{j+1}): {cap}')\n",
    "    \n",
    "# Show image\n",
    "imgs = tf.clip_by_value((imgs[:SAMPLE_NUM] + 1) / 2 * 255, 0, 255)\n",
    "img = generate_img(imgs, SAMPLE_ROW, SAMPLE_COL)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "optimizer_d = keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_PER_CKPT = 5\n",
    "\n",
    "checkpoint_path = './ckpt-bert3'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator=generator,\n",
    "                           discriminator=discriminator,\n",
    "                           optimizer_g=optimizer_g,\n",
    "                           optimizer_d=optimizer_d)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "start_epoch = 1\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    latest_ckpt = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    start_epoch = EPOCHS_PER_CKPT * latest_ckpt + 1\n",
    "    print(f'Restore from latest checkpoint: {ckpt_manager.latest_checkpoint.split(\"/\")[-1]}')\n",
    "    \n",
    "print(f'Start epoch: {start_epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "@tf.function\n",
    "def train_step_g(real_img, caption):\n",
    "    input_g = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "    text = caption\n",
    "\n",
    "    with tf.GradientTape() as tape_g:\n",
    "        fake_img = generator(text, input_g, training=True)\n",
    "        fake_pred = discriminator(text, fake_img, training=True)\n",
    "        loss_g = -tf.reduce_mean(fake_pred)\n",
    "        \n",
    "    gradient_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "    optimizer_g.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "    \n",
    "    return loss_g\n",
    "    \n",
    "@tf.function\n",
    "def train_step_d(real_img, caption):\n",
    "    input_g = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "    text = caption\n",
    "    epsilon = tf.random.uniform(shape=[BATCH_SIZE, 1, 1, 1], minval=0, maxval=1)\n",
    "    \n",
    "    with tf.GradientTape() as tape_d:\n",
    "        with tf.GradientTape() as tape_gp:\n",
    "            fake_img = generator(text, input_g, training=True)\n",
    "            fake_img_gp = epsilon * real_img + (1 - epsilon) * fake_img\n",
    "            fake_pred_gp = discriminator(text, fake_img_gp, training=True)\n",
    "        \n",
    "        gradient_gp = tape_gp.gradient(fake_pred_gp, fake_img_gp)\n",
    "        gradient_norm_gp = tf.sqrt(tf.reduce_sum(tf.square(gradient_gp), axis=np.arange(1, len(gradient_gp.shape))))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(gradient_norm_gp - 1))\n",
    "        \n",
    "        fake_pred = discriminator(text, fake_img, training=True)\n",
    "        real_pred = discriminator(text, real_img, training=True)\n",
    "        \n",
    "        loss_d = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred) + LAMBDA * gradient_penalty\n",
    "    \n",
    "    gradient_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "    optimizer_d.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return loss_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption):\n",
    "    input_g = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "    text = caption\n",
    "    fake_img = generator(text, input_g, training=False)\n",
    "    return fake_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_cap = [\n",
    "    'flower with white long white petals and very long purple stamen ',\n",
    "    'this medium white flower has rows of thin blue petals and thick stamen ',\n",
    "    'this flower is white and purple in color with petals that are oval shaped ',\n",
    "    'this flower is pink and yellow in color with petals that are oval shaped ',\n",
    "    'the flower has a large bright orange petal with pink anther ',\n",
    "    'the flower shown has a smooth white petal with patches of yellow as well ',\n",
    "    'white petals that become yellow as they go to the center where there is an orange stamen ',\n",
    "    'this flower has bright red petals with green pedicel as its main features ',\n",
    "    'this flower has the overlapping yellow petals arranged closely toward the center ',\n",
    "    'this flower has green sepals surrounding several layers of slightly ruffled pink petals ',\n",
    "    'the pedicel on this flower is purple with a green sepal and rose colored petals ',\n",
    "    'this white flower has connected circular petals with yellow stamen ',\n",
    "    'the flower has yellow petals overlapping each other and are yellow in color ',\n",
    "    'this flower has numerous stamen ringed by multiple layers of thin pink petals ',\n",
    "    'the petals are broad but thin at the edges with purple tints at the edges and white in the middle ',\n",
    "    'the yellow flower has petals that are soft smooth and arranged in two layers below the bunch of stamen ',\n",
    "    'this flower has petals that are pink and yellow with yellow stamen ',\n",
    "    'red stacked petals surround yellow stamen and a black pistil ',\n",
    "    'the petals of the flower are in multiple layers and are pink in yellow in color ',\n",
    "    'this flower has a yellow center and layers of peach colored petals with pointed tips ',\n",
    "    'this bright pink flower has several fluttery petals and a tubular center ',\n",
    "    'this flower is white and yellow in color with petals that are rounded at the endges ',\n",
    "    'the flower has a several pieces of yellow colored petals that looks similar to its leaves ',\n",
    "    'this flower has several light pink petals and yellow anthers ',\n",
    "    'this flower is yellow and white in color with petals that are star shaped near the cener ',\n",
    "    'lavender and white pedal and yellow small flower in the middle of the pedals ',\n",
    "    'this flower has lavender petals with maroon stripes and brown anther filaments ',\n",
    "    'this flower has six plain pale yellow petals that alternate with three dark yellow speckled petals ',\n",
    "    'this flower has petals that are yellow with orange lines ',\n",
    "    'the flower has petals that are orange with yellow stamen ',\n",
    "    'this flower has a brown center surrounded by layers of long yellow petals with rounded tips ',\n",
    "    'this flower is lavender in color with petals that are ruffled and wavy ',\n",
    "    'this flower is blue in color with petals that have veins ',\n",
    "    'the petals on this flower are white with yellow stamen ',\n",
    "    'this flower has petals that are cone shaped and dark purple ',\n",
    "    'this flower is purple and white in color and has petals that are multi colored ',\n",
    "    'a large group of bells that are blue on this flower ',\n",
    "    'this flower is bright purple with many pedals that are roundish whth pale white outer petals ',\n",
    "    'this flower has large yellow petals and long yellow stamen on it ',\n",
    "    'this flower has spiky blue petals and a spiky black stigma on it ',\n",
    "    'this flower is purple and yellow in color with petals that are oval shaped ',\n",
    "    'the flower has petals that are large and pink with yellow anther',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return index, caption\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = np.asarray(captions)\n",
    "#     caption = caption.astype(np.int32)\n",
    "    caption = [sent2WordList(x) for x in caption]\n",
    "    caption = tf.concat(\n",
    "        [text_encoder(caption[64 * i:64 * min(len(caption), i + 1)]) \\\n",
    "             for i in range((len(caption) + 63) // 64)], \n",
    "        axis=0)\n",
    "    caption = caption.numpy()\n",
    "    caption = np.asarray(caption)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    assert caption.shape[0] == index.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing_dataset  = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./inference/demo'):\n",
    "    os.makedirs('./inference/demo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for idx, captions in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions)\n",
    "        step += 1\n",
    "        for i in range(BATCH_SIZE):\n",
    "            img = np.clip(fake_image[i].numpy() * 0.5 + 0.5, 0.0, 1.0)\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), img)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_command_with_epoch(epoch):\n",
    "    \"\"\"Run the command in the 'testing' directory and save results with an epoch postfix.\"\"\"\n",
    "    command_base = [\"python\", \"inception_score.py\", \"../inference/demo\"]\n",
    "    output_file = f\"../score_demo_epoch_{epoch:04d}.csv\"\n",
    "    command = command_base + [output_file, \"39\"]\n",
    "\n",
    "    try:\n",
    "        # Execute the command\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Command executed successfully, output saved to {output_file}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed with error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def eval_score(file_path):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Calculate the average of the 'score' column\n",
    "    average_score = data['score'].mean()\n",
    "\n",
    "    # Print the average score\n",
    "    print(f'Average score: {average_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'):\n",
    "    os.makedirs('samples/demo')\n",
    "\n",
    "EPOCHS = 2000\n",
    "N_CRITIC = 3\n",
    "\n",
    "loss_x_list = []\n",
    "loss_g_list = []\n",
    "loss_d_list = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sample_z = tf.random.normal([SAMPLE_NUM, Z_DIM])\n",
    "sample_cap = visual_cap[:SAMPLE_NUM]\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    \n",
    "    loss_g = 0\n",
    "    loss_d = 0\n",
    "    \n",
    "    # Train step\n",
    "    for step, (img, cap) in tqdm(enumerate(dataset), total=num_steps):\n",
    "        if step % (N_CRITIC + 1) == N_CRITIC - 1:\n",
    "            loss_g += train_step_g(img, cap)\n",
    "            # Store data to list\n",
    "            loss_x_list.append(epoch + step / num_steps)\n",
    "            loss_g_list.append(loss_g / 1)\n",
    "            loss_d_list.append(loss_d / N_CRITIC)\n",
    "            loss_g = 0\n",
    "            loss_d = 0\n",
    "        else:\n",
    "            loss_d += train_step_d(img, cap)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch % EPOCHS_PER_CKPT == 0:\n",
    "        ckpt_manager.save()\n",
    "    \n",
    "    # Generate sample image\n",
    "    sample_text = text_encoder(sample_cap)\n",
    "    sample_imgs = generator(sample_text, sample_z, training=False)\n",
    "    sample_imgs = tf.clip_by_value((sample_imgs + 1) / 2 * 255, 0, 255)\n",
    "    img = generate_img(sample_imgs, SAMPLE_ROW, SAMPLE_COL, f'imgs/{epoch:>04d}.png')\n",
    "    \n",
    "    # Display sample image\n",
    "    if epoch % 5 == 0:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        inference(testing_dataset)\n",
    "        \n",
    "        # Change to the testing directory\n",
    "        testing_dir = \"./testing\"\n",
    "        original_dir = os.getcwd()  # Save the current directory\n",
    "        os.chdir(testing_dir)\n",
    "        # print(f\"Changed directory to {testing_dir}. Running command: {' '.join(command)}\")\n",
    "        \n",
    "        # Run command with the current epoch\n",
    "        output_file = run_command_with_epoch(epoch)\n",
    "        # Evaluate the score\n",
    "        eval_score(output_file)\n",
    "\n",
    "        # Change back to the original directory\n",
    "        os.chdir(original_dir)\n",
    "        # print(f\"Returned to the original directory: {original_dir}.\")\n",
    "\n",
    "    print(f'Epoch {epoch}: Loss_g {loss_g_list[-1]:.5f}, Loss_d {loss_d_list[-1]:.5f}')\n",
    "    \n",
    "print (f'Time taken for {EPOCHS} epoch: {time.time() - start} sec')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
